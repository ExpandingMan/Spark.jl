<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · Spark</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Spark</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Introduction</a></li><li class="current"><a class="toctext" href="api.html">API Reference</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href="api.html">API Reference</a></li></ul><a class="edit-page" href="https://github.com/dfdx/Spark.jl/tree/66cd7e089074d56c34d26fe66d6d896ce92d783e/api.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>API Reference</span><a class="fa fa-bars" href="#"></a></div></header><ul><li><a href="api.html#Spark.FlatMapIterator"><code>Spark.FlatMapIterator</code></a></li><li><a href="api.html#Spark.JavaPairRDD"><code>Spark.JavaPairRDD</code></a></li><li><a href="api.html#Spark.JavaRDD"><code>Spark.JavaRDD</code></a></li><li><a href="api.html#Spark.PipelinedPairRDD-Tuple{Spark.RDD,Function}"><code>Spark.PipelinedPairRDD</code></a></li><li><a href="api.html#Spark.PipelinedPairRDD"><code>Spark.PipelinedPairRDD</code></a></li><li><a href="api.html#Spark.PipelinedRDD-Tuple{Spark.RDD,Function}"><code>Spark.PipelinedRDD</code></a></li><li><a href="api.html#Spark.PipelinedRDD"><code>Spark.PipelinedRDD</code></a></li><li><a href="api.html#Spark.SparkContext-Tuple{}"><code>Spark.SparkContext</code></a></li><li><a href="api.html#Spark.SparkContext"><code>Spark.SparkContext</code></a></li><li><a href="api.html#Base.close-Tuple{Spark.SparkContext}"><code>Base.close</code></a></li><li><a href="api.html#Base.collect-Tuple{Spark.SingleRDD}"><code>Base.collect</code></a></li><li><a href="api.html#Base.collect-Tuple{Spark.PairRDD}"><code>Base.collect</code></a></li><li><a href="api.html#Base.count-Tuple{Spark.RDD}"><code>Base.count</code></a></li><li><a href="api.html#Base.map-Tuple{Spark.RDD,Function}"><code>Base.map</code></a></li><li><a href="api.html#Base.reduce-Tuple{Spark.RDD,Function}"><code>Base.reduce</code></a></li><li><a href="api.html#Spark.add_file-Tuple{Spark.SparkContext,AbstractString}"><code>Spark.add_file</code></a></li><li><a href="api.html#Spark.add_jar-Tuple{Spark.SparkContext,AbstractString}"><code>Spark.add_jar</code></a></li><li><a href="api.html#Spark.cache-Tuple{Spark.PairRDD}"><code>Spark.cache</code></a></li><li><a href="api.html#Spark.cache-Tuple{Spark.SingleRDD}"><code>Spark.cache</code></a></li><li><a href="api.html#Spark.cartesian-Tuple{Spark.SingleRDD,Spark.SingleRDD}"><code>Spark.cartesian</code></a></li><li><a href="api.html#Spark.chain_function-Tuple{Any,Any}"><code>Spark.chain_function</code></a></li><li><a href="api.html#Spark.coalesce-Tuple{T&lt;:Spark.RDD,Integer}"><code>Spark.coalesce</code></a></li><li><a href="api.html#Spark.collect_internal-Tuple{Spark.RDD,Any,Any}"><code>Spark.collect_internal</code></a></li><li><a href="api.html#Spark.collect_internal_itr-Tuple{Spark.RDD,Any,Any}"><code>Spark.collect_internal_itr</code></a></li><li><a href="api.html#Spark.collect_itr-Tuple{Spark.PairRDD}"><code>Spark.collect_itr</code></a></li><li><a href="api.html#Spark.collect_itr-Tuple{Spark.SingleRDD}"><code>Spark.collect_itr</code></a></li><li><a href="api.html#Spark.context-Tuple{Spark.RDD}"><code>Spark.context</code></a></li><li><a href="api.html#Spark.create_flat_map_function-Tuple{Function}"><code>Spark.create_flat_map_function</code></a></li><li><a href="api.html#Spark.create_map_function-Tuple{Function}"><code>Spark.create_map_function</code></a></li><li><a href="api.html#Spark.deserialized-Tuple{Array{UInt8,1}}"><code>Spark.deserialized</code></a></li><li><a href="api.html#Spark.flat_map-Tuple{Spark.RDD,Function}"><code>Spark.flat_map</code></a></li><li><a href="api.html#Spark.flat_map_pair-Tuple{Spark.RDD,Function}"><code>Spark.flat_map_pair</code></a></li><li><a href="api.html#Spark.group_by_key-Tuple{Spark.PairRDD}"><code>Spark.group_by_key</code></a></li><li><a href="api.html#Spark.id-Tuple{Spark.RDD}"><code>Spark.id</code></a></li><li><a href="api.html#Spark.map_pair-Tuple{Spark.RDD,Function}"><code>Spark.map_pair</code></a></li><li><a href="api.html#Spark.map_partitions-Tuple{Spark.RDD,Function}"><code>Spark.map_partitions</code></a></li><li><a href="api.html#Spark.map_partitions_pair-Tuple{Spark.RDD,Function}"><code>Spark.map_partitions_pair</code></a></li><li><a href="api.html#Spark.map_partitions_with_index-Tuple{Spark.RDD,Function}"><code>Spark.map_partitions_with_index</code></a></li><li><a href="api.html#Spark.num_partitions-Tuple{Union{Spark.PipelinedPairRDD,Spark.PipelinedRDD}}"><code>Spark.num_partitions</code></a></li><li><a href="api.html#Spark.pipe-Tuple{Spark.RDD,String}"><code>Spark.pipe</code></a></li><li><a href="api.html#Spark.readobj-Tuple{IO}"><code>Spark.readobj</code></a></li><li><a href="api.html#Spark.reduce_by_key-Tuple{Spark.PairRDD,Function}"><code>Spark.reduce_by_key</code></a></li><li><a href="api.html#Spark.repartition-Tuple{T&lt;:Spark.RDD,Integer}"><code>Spark.repartition</code></a></li><li><a href="api.html#Spark.serialized-Tuple{Any}"><code>Spark.serialized</code></a></li><li><a href="api.html#Spark.share_variable-Tuple{Spark.SparkContext,Symbol,Any}"><code>Spark.share_variable</code></a></li><li><a href="api.html#Spark.text_file-Tuple{Spark.SparkContext,AbstractString}"><code>Spark.text_file</code></a></li><li><a href="api.html#Spark.writeobj-Tuple{IO,Any}"><code>Spark.writeobj</code></a></li></ul><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.JavaRDD" href="#Spark.JavaRDD"><code>Spark.JavaRDD</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Pure wrapper around JavaRDD</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.SparkContext" href="#Spark.SparkContext"><code>Spark.SparkContext</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Wrapper around JavaSparkContext</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.SparkContext-Tuple{}" href="#Spark.SparkContext-Tuple{}"><code>Spark.SparkContext</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Params:</p><ul><li><p>master - address of application master. Currently only local and standalone modes          are supported. Default is &#39;local&#39;</p></li><li><p>appname - name of application</p></li></ul></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.close-Tuple{Spark.SparkContext}" href="#Base.close-Tuple{Spark.SparkContext}"><code>Base.close</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Close SparkContext</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.collect-Tuple{Spark.PairRDD}" href="#Base.collect-Tuple{Spark.PairRDD}"><code>Base.collect</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Collect all elements of <code>rdd</code> on a driver machine</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.collect-Tuple{Spark.SingleRDD}" href="#Base.collect-Tuple{Spark.SingleRDD}"><code>Base.collect</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Collect all elements of <code>rdd</code> on a driver machine</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.count-Tuple{Spark.RDD}" href="#Base.count-Tuple{Spark.RDD}"><code>Base.count</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Count number of elements in this RDD</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.map-Tuple{Spark.RDD,Function}" href="#Base.map-Tuple{Spark.RDD,Function}"><code>Base.map</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Apply function <code>f</code> to each element of <code>rdd</code></p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.reduce-Tuple{Spark.RDD,Function}" href="#Base.reduce-Tuple{Spark.RDD,Function}"><code>Base.reduce</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Reduce elements of <code>rdd</code> using specified function <code>f</code></p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.cache-Tuple{Spark.PairRDD}" href="#Spark.cache-Tuple{Spark.PairRDD}"><code>Spark.cache</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Persist this RDD with the default storage level (MEMORY_ONLY)</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.cache-Tuple{Spark.SingleRDD}" href="#Spark.cache-Tuple{Spark.SingleRDD}"><code>Spark.cache</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Persist this RDD with the default storage level (MEMORY_ONLY)</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.cartesian-Tuple{Spark.SingleRDD,Spark.SingleRDD}" href="#Spark.cartesian-Tuple{Spark.SingleRDD,Spark.SingleRDD}"><code>Spark.cartesian</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Create a pair RDD with every combination of the values of rdd1 and rdd2</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.coalesce-Tuple{T&lt;:Spark.RDD,Integer}" href="#Spark.coalesce-Tuple{T&lt;:Spark.RDD,Integer}"><code>Spark.coalesce</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Return a new RDD that is reduced into num_partitions partitions.</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.flat_map-Tuple{Spark.RDD,Function}" href="#Spark.flat_map-Tuple{Spark.RDD,Function}"><code>Spark.flat_map</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Similar to <code>map</code>, but each input item can be mapped to 0 or more output items (so <code>f</code> should return an iterator rather than a single item)</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.flat_map_pair-Tuple{Spark.RDD,Function}" href="#Spark.flat_map_pair-Tuple{Spark.RDD,Function}"><code>Spark.flat_map_pair</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Similar to <code>map</code>, but each input item can be mapped to 0 or more output items (so <code>f</code> should return an iterator of pairs rather than a single item)</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.group_by_key-Tuple{Spark.PairRDD}" href="#Spark.group_by_key-Tuple{Spark.PairRDD}"><code>Spark.group_by_key</code></a> — <span class="docstring-category">Method</span>.</div><div><p>When called on a dataset of (K, V) pairs, returns a dataset of (K, [V]) pairs.</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.id-Tuple{Spark.RDD}" href="#Spark.id-Tuple{Spark.RDD}"><code>Spark.id</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Return the id of the rdd</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.map_pair-Tuple{Spark.RDD,Function}" href="#Spark.map_pair-Tuple{Spark.RDD,Function}"><code>Spark.map_pair</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Apply function <code>f</code> to each element of <code>rdd</code></p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.map_partitions-Tuple{Spark.RDD,Function}" href="#Spark.map_partitions-Tuple{Spark.RDD,Function}"><code>Spark.map_partitions</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Apply function <code>f</code> to each partition of <code>rdd</code>. <code>f</code> should be of type <code>(iterator) -&gt; iterator</code></p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.map_partitions_pair-Tuple{Spark.RDD,Function}" href="#Spark.map_partitions_pair-Tuple{Spark.RDD,Function}"><code>Spark.map_partitions_pair</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Apply function <code>f</code> to each partition of <code>rdd</code>. <code>f</code> should be of type <code>(iterator) -&gt; iterator</code></p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.map_partitions_with_index-Tuple{Spark.RDD,Function}" href="#Spark.map_partitions_with_index-Tuple{Spark.RDD,Function}"><code>Spark.map_partitions_with_index</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Apply function <code>f</code> to each partition of <code>rdd</code>. <code>f</code> should be of type <code>(index, iterator) -&gt; iterator</code></p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.num_partitions-Tuple{Union{Spark.PipelinedPairRDD,Spark.PipelinedRDD}}" href="#Spark.num_partitions-Tuple{Union{Spark.PipelinedPairRDD,Spark.PipelinedRDD}}"><code>Spark.num_partitions</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Returns the number of partitions of this RDD.</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.pipe-Tuple{Spark.RDD,String}" href="#Spark.pipe-Tuple{Spark.RDD,String}"><code>Spark.pipe</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Return an RDD created by piping elements to a forked external process.</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.reduce_by_key-Tuple{Spark.PairRDD,Function}" href="#Spark.reduce_by_key-Tuple{Spark.PairRDD,Function}"><code>Spark.reduce_by_key</code></a> — <span class="docstring-category">Method</span>.</div><div><p>When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) =&gt; V.</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.repartition-Tuple{T&lt;:Spark.RDD,Integer}" href="#Spark.repartition-Tuple{T&lt;:Spark.RDD,Integer}"><code>Spark.repartition</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Return a new RDD that has exactly num_partitions partitions.</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.share_variable-Tuple{Spark.SparkContext,Symbol,Any}" href="#Spark.share_variable-Tuple{Spark.SparkContext,Symbol,Any}"><code>Spark.share_variable</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Makes the value of data available on workers as symbol name</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.text_file-Tuple{Spark.SparkContext,AbstractString}" href="#Spark.text_file-Tuple{Spark.SparkContext,AbstractString}"><code>Spark.text_file</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Create RDD from a text file</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.FlatMapIterator" href="#Spark.FlatMapIterator"><code>Spark.FlatMapIterator</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Iterates over the iterators within an iterator</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.JavaPairRDD" href="#Spark.JavaPairRDD"><code>Spark.JavaPairRDD</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Pure wrapper around JavaPairRDD</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.PipelinedPairRDD" href="#Spark.PipelinedPairRDD"><code>Spark.PipelinedPairRDD</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Julia type to handle Pair RDDs. Can handle pipelining of operations to reduce interprocess IO.</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.PipelinedPairRDD-Tuple{Spark.RDD,Function}" href="#Spark.PipelinedPairRDD-Tuple{Spark.RDD,Function}"><code>Spark.PipelinedPairRDD</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Params:</p><ul><li><p>parentrdd - parent RDD</p></li><li><p>func - function of type <code>(index, iterator) -&gt; iterator</code> to apply to each partition</p></li></ul></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.PipelinedRDD" href="#Spark.PipelinedRDD"><code>Spark.PipelinedRDD</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Julia type to handle RDDs. Can handle pipelining of operations to reduce interprocess IO.</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.PipelinedRDD-Tuple{Spark.RDD,Function}" href="#Spark.PipelinedRDD-Tuple{Spark.RDD,Function}"><code>Spark.PipelinedRDD</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Params:</p><ul><li><p>parentrdd - parent RDD</p></li><li><p>func - function of type <code>(index, iterator) -&gt; iterator</code> to apply to each partition</p></li></ul></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.add_file-Tuple{Spark.SparkContext,AbstractString}" href="#Spark.add_file-Tuple{Spark.SparkContext,AbstractString}"><code>Spark.add_file</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Add file to SparkContext. This file will be downloaded to each executor&#39;s work directory</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.add_jar-Tuple{Spark.SparkContext,AbstractString}" href="#Spark.add_jar-Tuple{Spark.SparkContext,AbstractString}"><code>Spark.add_jar</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Add JAR file to SparkContext. Classes from this JAR will then be available to all tasks</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.chain_function-Tuple{Any,Any}" href="#Spark.chain_function-Tuple{Any,Any}"><code>Spark.chain_function</code></a> — <span class="docstring-category">Method</span>.</div><div><p>chain 2 partion functions together </p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.collect_internal-Tuple{Spark.RDD,Any,Any}" href="#Spark.collect_internal-Tuple{Spark.RDD,Any,Any}"><code>Spark.collect_internal</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Collects the RDD to the Julia process, by serialising all values via a byte array</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.collect_internal_itr-Tuple{Spark.RDD,Any,Any}" href="#Spark.collect_internal_itr-Tuple{Spark.RDD,Any,Any}"><code>Spark.collect_internal_itr</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Collects the RDD to the Julia process, via an Julia iterator that fetches each row at a time. This prevents creation of a byte array containing all rows at a time.</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.collect_itr-Tuple{Spark.PairRDD}" href="#Spark.collect_itr-Tuple{Spark.PairRDD}"><code>Spark.collect_itr</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Collect all elements of <code>rdd</code> on a driver machine</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.collect_itr-Tuple{Spark.SingleRDD}" href="#Spark.collect_itr-Tuple{Spark.SingleRDD}"><code>Spark.collect_itr</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Collect all elements of <code>rdd</code> on a driver machine</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.context-Tuple{Spark.RDD}" href="#Spark.context-Tuple{Spark.RDD}"><code>Spark.context</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Get SparkContext of this RDD</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.create_flat_map_function-Tuple{Function}" href="#Spark.create_flat_map_function-Tuple{Function}"><code>Spark.create_flat_map_function</code></a> — <span class="docstring-category">Method</span>.</div><div><p>creates a function that operates on a partition from an element by element flat_map function</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.create_map_function-Tuple{Function}" href="#Spark.create_map_function-Tuple{Function}"><code>Spark.create_map_function</code></a> — <span class="docstring-category">Method</span>.</div><div><p>creates a function that operates on a partition from an element by element map function</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.deserialized-Tuple{Array{UInt8,1}}" href="#Spark.deserialized-Tuple{Array{UInt8,1}}"><code>Spark.deserialized</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Return object deserialized from array of bytes</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.readobj-Tuple{IO}" href="#Spark.readobj-Tuple{IO}"><code>Spark.readobj</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Read data object from a ioet. Returns code and byte array:</p><ul><li><p>if code is negative, it&#39;s considered as a special command code</p></li><li><p>if code is positive, it&#39;s considered as array length</p></li></ul></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.serialized-Tuple{Any}" href="#Spark.serialized-Tuple{Any}"><code>Spark.serialized</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Return serialized object as an array of bytes</p></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Spark.writeobj-Tuple{IO,Any}" href="#Spark.writeobj-Tuple{IO,Any}"><code>Spark.writeobj</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Write object to stream</p></div></section><footer><hr/><a class="previous" href="index.html"><span class="direction">Previous</span><span class="title">Introduction</span></a></footer></article></body></html>
